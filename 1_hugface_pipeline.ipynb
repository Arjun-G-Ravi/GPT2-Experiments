{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 using huggingface library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do inference with huggingface pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           TextGenerationPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x791a94344490>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/AI_ENV/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
      "specified text prompt. When the underlying model is a conversational model, it can also accept one or more chats,\n",
      "in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).\n",
      "Each chat takes the form of a list of dicts, where each dict contains \"role\" and \"content\" keys.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> generator = pipeline(model=\"openai-community/gpt2\")\n",
      ">>> generator(\"I can't believe you did such a \", do_sample=False)\n",
      "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
      "\n",
      ">>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
      ">>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
      "```\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> generator = pipeline(model=\"HuggingFaceH4/zephyr-7b-beta\")\n",
      ">>> # Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string\n",
      ">>> generator([{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}], do_sample=False, max_new_tokens=2)\n",
      "[{'generated_text': [{'role': 'user', 'content': 'What is the capital of France? Answer in one word.'}, {'role': 'assistant', 'content': 'Paris'}]}]\n",
      "```\n",
      "\n",
      "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
      "generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
      "text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
      "generation](text_generation).\n",
      "\n",
      "This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      "`\"text-generation\"`.\n",
      "\n",
      "The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
      "objective. See the list of available [text completion models](https://huggingface.co/models?filter=text-generation)\n",
      "and the list of [conversational models](https://huggingface.co/models?other=conversational)\n",
      "on [huggingface.co/models].\n",
      "\n",
      "Arguments:\n",
      "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      "    tokenizer ([`PreTrainedTokenizer`]):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      "        [`PreTrainedTokenizer`].\n",
      "    modelcard (`str` or [`ModelCard`], *optional*):\n",
      "        Model card attributed to the model for this pipeline.\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed.\n",
      "\n",
      "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "        provided.\n",
      "    task (`str`, defaults to `\"\"`):\n",
      "        A task-identifier for the pipeline.\n",
      "    num_workers (`int`, *optional*, defaults to 8):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      "        workers to be used.\n",
      "    batch_size (`int`, *optional*, defaults to 1):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
      "    device (`int`, *optional*, defaults to -1):\n",
      "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      "        the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "        (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      "    binary_output (`bool`, *optional*, defaults to `False`):\n",
      "        Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      "        the raw output data e.g. text.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Complete the prompt(s) given as inputs.\n",
      "\n",
      "Args:\n",
      "    text_inputs (`str`, `List[str]`, List[Dict[str, str]], or `List[List[Dict[str, str]]]`):\n",
      "        One or several prompts (or one list of prompts) to complete. If strings or a list of string are\n",
      "        passed, this pipeline will continue each prompt. Alternatively, a \"chat\", in the form of a list\n",
      "        of dicts with \"role\" and \"content\" keys, can be passed, or a list of such chats. When chats are passed,\n",
      "        the model's chat template will be used to format them before passing them to the model.\n",
      "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to\n",
      "        `True`, the decoded text is not returned.\n",
      "    return_text (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to return the decoded texts in the outputs.\n",
      "    return_full_text (`bool`, *optional*, defaults to `True`):\n",
      "        If set to `False` only added text is returned, otherwise the full text is returned. Only meaningful if\n",
      "        *return_text* is set to True.\n",
      "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to clean up the potential extra spaces in the text output.\n",
      "    prefix (`str`, *optional*):\n",
      "        Prefix added to prompt.\n",
      "    handle_long_generation (`str`, *optional*):\n",
      "        By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
      "        the model maximum length). There is no perfect way to adress this (more info\n",
      "        :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
      "        strategies to work around that problem depending on your use case.\n",
      "\n",
      "        - `None` : default strategy where nothing in particular happens\n",
      "        - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
      "          truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
      "    generate_kwargs (`dict`, *optional*):\n",
      "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
      "        corresponding to your framework [here](./main_classes/text_generation)).\n",
      "\n",
      "Return:\n",
      "    A list or a list of lists of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
      "    of both `generated_text` and `generated_token_ids`):\n",
      "\n",
      "    - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
      "    - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
      "      ids of the generated text."
     ]
    }
   ],
   "source": [
    "generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I can't believe you did such a \", do_sample=False) # output is prone to repetition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
